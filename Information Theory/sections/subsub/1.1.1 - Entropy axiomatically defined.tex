\documentclass{subfiles}
\begin{document}
    In \emph{Section \ref{Sec:1.1}} we stated that the only form \(H\) can 
    assume is the one of \emph{Equation \eqref{Eq:1}}. 
    The reason behind this, though one may use different approaches 
        (eg. the connection with the entropy in quantum mechanics), 
        is the way entropy is defined axiomatically. 

    \begin{axiom*}
        Let \(H\) be a measure of uncertainty of a source \(S\), 
            then \(H\) must satisfy the following properties.
        \begin{enumerate}
            \item \(H\) must be a continuos function of the probabilities.
                Meaning that if \(H(S) = \entropy{p_{1}, \ldots, p_{n}}\), 
                then \(H\) is continuous in [0, 1].

            \item \(\entropy{\sfrac{1}{2}, \sfrac{1}{2}}[2] = 1\).

            \item If \(\tau\) is a permutation of the probabilities, 
                then \(\entropy{\tau} = \entropy{p_{1}, \ldots, p_{n}}[n]\). 
                Meaning that, \(H\) must not care of the order of the probabilities.

            \item If \(A(n) = \entropy{\sfrac{1}{n}, \ldots, \sfrac{1}{n}}[n]\),
                then \(H\) is monotonically increasing.
                Equally probable events imply greater uncertainty.

            \item If a choice can be divided into \(k\) consecutive ones,
                then the original value of \(H\) must be the weighted sum of the new \(H\).
                Meaning that
                \[\begin{aligned}
                    \entropy{p_{1}, \ldots, p_{n}}[n] & = 
                        \entropy{
                            p_{1} + \ldots + p_{k}, p_{k + 1}, \ldots, p_{n}
                        }[n - k + 1] \\ 
                        & + (p_{1} + \ldots + p_{k})
                            \entropy{
                                \frac{p_{1}}{p_{1} + \ldots + p_{k}},
                                \ldots,
                                \frac{p_{k}}{p_{1} + \ldots + p_{k}}
                            }[k].
                \end{aligned}\]
       \end{enumerate}
    \end{axiom*} 

    \begin{theorem*}[Theorem 2, \cite{shannon1948}]
        The only function \(H\) that satisfies the above axioms,
        has the form of \emph{Equation \eqref{Eq:1}}.
    \end{theorem*}
    \begin{proof*}
        Let's show how \emph{Equation \eqref{Eq:1}} satisfies all the axioms.
        \begin{enumerate}
            \item From axiom 5 we have \(A(nm) = A(n) + A(m)\).
                Let's note in fact that: 
                \[\begin{aligned}
                    A(nm) & = \entropy{\frac{1}{nm}, \ldots, \frac{1}{nm}}[nm] \\ 
                        & = \entropy{
                            \frac{1}{m}, \frac{1}{nm}, \ldots, \frac{1}{nm} 
                        }[nm - n + 1] + \frac{1}{m} \entropy{
                            \frac{1}{n}, \ldots, \frac{1}{n}
                        }[n] \\
                        & \underbrace{ = \ldots = }_{m \text{ times}} \entropy{
                            \frac{1}{m}, \ldots, \frac{1}{m}
                        }[m] + \entropy{\frac{1}{n}, \ldots, \frac{1}{n}}[n] \\ 
                    & = A(m) + A(n).
                \end{aligned}\]
            \item From the previous point, 
                we also have that \(A(n^{k}) = k A(n)\), for any \(n, k\).

            \item Let us consider some \(r\) big enough such that 
                \(\exists k : 2^{k} \le n^{r} < 2^{k + 1}\).
                It follows that 
                \[
                    k \log_{b} 2 \le r \log_{b} n < (k + 1) \log_{b} 2.
                \]
                Let's divide everything by \(r \log_{b} 2\), we now have 
                \[
                    \frac{k}{r} \le \log_{2} n < \frac{k + 1}{r} 
                        \implies \abs[\log_{2} n - \frac{k}{r}] < \frac{1}{r}.
                \]
                From axiom 4 follows \(A(2^{k}) \le A(n^{r}) < A(n^{k + 1})\),
                which means that \(k A(2) \le r A(n) < (k + 1)A(2)\).
                From a similar reasoning as before, we get 
                \[
                    \abs[\frac{A(n)}{A(2)} - \frac{k}{r}] < \frac{1}{r}
                        \implies A(n) \approx \frac{A(2)}{\log_{b} 2} \log_{b} n
                \]
                since \(\sfrac{A(n)}{A(2)} \approx \log_{b} n\).
                Thus, \(A(n) = c \log_{b} n\).

            \item Suppose \(p \in \Q \iff p = \sfrac{r}{s}, r \le s\).
                Then we have 
                \[\begin{aligned}
                    A(s) & = \entropy{\frac{1}{s}, \ldots, \frac{1}{s}}[s] \\ 
                        & = \entropy{\frac{r}{s}, \frac{1}{s}, \ldots, \frac{1}{s}}[s - r + 1]
                          + \frac{r}{s} \entropy{\frac{1}{r}, \ldots, \frac{1}{r}}[r] \\ 
                        & = \ldots = \entropy{\frac{r}{s}, 1 - \frac{r}{s}}[2]
                          + \frac{s - r}{s} A(s - r) + \frac{r}{s} A(s).
                \end{aligned}\]
                From the previous point, we can conclude that 
                \[
                    A(n) = -c \lbrack
                        (1 - p) \log_{b} (1 - p) + p \log_{b} p
                    \rbrack.
                \]
                Also, since \(\Q \text{ is dense in } \R\) and \(H\) is continuous,
                we can extend what above to any \(p \in \R\).

            \item Lastly, \emph{Equation \eqref{Eq:1}} can be easily proved to hold,
                by induction on \(n\).

        \end{enumerate}
    \end{proof*}
    The proof we provide is berely a sketch.
    See \cite{shannon1948} (\emph{Appendix 2}) for a detailed proof.
\end{document}
