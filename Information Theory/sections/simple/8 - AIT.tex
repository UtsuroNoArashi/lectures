\documentclass{subfiles}
\begin{document}
    At the beginning of these notes we said that information theory provides a
        toolbox one can use to understand the limits of what is computable.
        In truth, such toolbox come from an extension of the information theory,
        known as algorithmic information theory. The latter, in fact,
        deals with information but from a randomness and complexity stand point.

    The theory was initially proposed by the Soviet mathematician \emph{Andrey Kolmogorov} in the 1960s.
        In \cite{kolmogorov1965}, Kolmogorov remarks some limitations of Shannon's entropy.
        In particular, his critique was about the fact that, for any given object,
        Shannon's information was defined in terms of the probability that such object 
        got produced by a source, rather than the object itself.
        Kolmogorov thus proposed to define the information of a given object
        by its complexity.

        \subsection{Kolmogorov complexity}
        \subfile{../sub/8.1 - Kolmogorov complexity}

        \subsection{Smallest grammar problem}
        \subfile{../sub/8.2 - SGP}
\end{document}
