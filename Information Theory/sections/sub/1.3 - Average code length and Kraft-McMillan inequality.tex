\documentclass{subfiles}
\begin{document}

    In the previous section, we introduced prefix codes. 
        The question now is: given two distinct prefix-free codes, which one is more efficient?
        A good choice is the code that, on average, has the shortest length.

    \begin{definition*}[\Gls{acl}]
        Let \(C\) be a code with source alphabet \(S = \set{s_{1}, \ldots, s_{n}}\)
            and code alphabet \(X = \set{x_{1}, \ldots, x_{m}}\). 
            Let \(\set{c_{1}, \ldots, c_{n}}\) be the codewords with lengths \(l_{1}, \ldots, l_{n}\), respectively.
            Let \(\set{p_{1}, \ldots, p_{n}}\) be the probabilities of the source symbols. 
            Then, we define the quantity
            \[
                L_{S}(C) = \sum_{i = 1}^{n}{p_{i} l_{i}}
            \]
            as the average code length of the code \(C\).
    \end{definition*}

    From the above, it makes sense to look for the UD code with the lowest average code length.
        That is, among all UD codes for the same source and with the same code alphabet,
        the one with the lowest ACL. But how do we find such codes?
        A good starting point is the entropy of the source. 
        Once again, thanks to Shannon, we have the following result.

    \begin{theorem*}[Shannon]
        Let \(C\) be a UD code for a memoryless source \(S\), 
        whose probabilities are \(\set{p_{1}, \ldots, p_{n}}\),
        and code alphabet \(X\) of size \(d\). 
        Then 
        \[
            L_{S}(C) \ge \frac{\entropy{S}}{\log_{b}{d}}
        \]
    \end{theorem*}

    \subsubsection{The Kraft-McMillan inequality}
    \subfile{../subsub/1.3.1 - Kraft-McMillan}

    \subsubsection{Optimal codes and the Shannon-Fano encoding}
    \subfile{../subsub/1.3.2 - Optimal codes and SF encoding}
\end{document}
