\documentclass{subfiles}
\begin{document}
    When talking about information we all have a general notion of what it represents,
        but can we define it formally? And also, is there a way to measure information?

    The first to answer these questions was \emph{Claude Shannon}, 
        by many condsidered the father of modern information theory.
    In \cite{ShannonEntropy} Shannon analizes various comunication systems:
        from the discrete noiceless one to the continous noisy one.
    A general structure of these systems is shown in \emph{Figure \ref{Fig:1}}.
    \subfile{../../extra/TikZ/Figure 1 - Source structure}

    \begin{itemize}
        \item The \emph{Source} or, more precisely, 
            the \emph{Information Source} referes to some entity
            (a human, a computer, etc) that produces messages.

        \item The \emph{Encoder} encodes the messages coming from the source,
            and transmits them trough the channel.
.
        \item The \emph{channel} is the physical mean trough which the messages
            are transmitted.

        \item The \emph{Decoder} has the opposite role of the \emph{Encoder}.
        \item The \emph{Destination} is the entity to which the messages are meant for.
    \end{itemize}

    \begin{remark*}
        What follows in this section, and the those following,
        refers just to the discrete noiceless case. 
        We also assume the source to be memoryless, 
        meaning that each symbol produced is indipendent from the previous one.
    \end{remark*}

    Let \(S\) be a source of information. 
    Let \(\Sigma\) be the alphabet of symbols used by the source,
    and for each symbols let \(p_{i} = \Pr(S = \sigma_{i})\) at any given time.
    What we are looking for is some function \(H\), if it exists,
    that measures the uncertainty we have about \(S\).

    As we will show in \emph{Section \ref{Sec:1.1.1}}, 
        the only form \(H\) can assume is the following:
    \begin{equation}\label{Eq:1}
        H = -K \sum_{i = 1}^{n = \Abs{\Sigma}}{p_{i} \log_{b} p_{i}}.
    \end{equation}
    where \(K\) is a positive constant and \(b \text{ is usually } 2\).

    We define \(H(S)\) to be the \emph{entropy} of \(S\). 
    Formally, the entropy measures the average amount of information stored 
        in each symbol of the source output.
        Thus, we can define the information as the quantity of uncertainty lost,
        once the outcome of a given event is known.
    \note{
        In this context, 
        by ``event'' we refer to the symbol produced by the source at a given time.
    }

    \subsubsection{Entropy: an axiomatic definition}\label{Sec:1.1.1}
    \subfile{../subsub/1.1.1 - Entropy axiomatically defined}
\end{document}
