\documentclass{subfiles}
\begin{document}
    In the previous section, we've introduced prefix codes. 
    The question now is, given two distinct prefix-free codes,
        which one is more convenient to use?
        A good choice would be the one that, on average,
        has the shortest length.

    \begin{definition*}[\Gls{acl}]
        Let \(C\) be a code with a source alphabet \(S = \Set{s_{1}, \ldots, s_{n}}\)
        and a code alphabet \(X = \Set{x_{1}, \ldots, x_{m}}\). 
        Let \(\Set{c_{1}, \ldots, c_{n}}\) be the codewords with lengths 
        \(l_{1}, \ldots, l_{n}\) respectively. And let \(\Set{p_{1}, \ldots, p_{n}}\)
        be the probabilities for the source symbols. 
            Then, we define the quantity
            \[
                L_{S}(C) = \sum_{i = 1}^{m}{p_{i} l_{i}}
            \]
            as the average code length of the code \(C\).
    \end{definition*}
    From what above, it makes sense to look for the \gls{ud} code with the lowest average code length.
    That is, among all the \gls{ud} codes for the same source and with the same code alphabet,
    the one with the lowest \gls{acl}. But how do we find such codes?
    A good starting point is the entropy of the source. Once again, 
    thanks to Shannon, we have the following result.
    \begin{theorem*}[Shannon]
        Let \(C\) be a \gls{ud} code for a memoryless source \(S\),
        whose probabilities are \(\Set{p_{1}, \ldots, p_{n}}\),
        and alphabet code \(X\) with a size \(d\). 
        Then 
        \[
            L_{S}(C) \ge \frac{\Entropy{S}}{\log_{b}{d}}
        \]
    \end{theorem*}

    \subsubsection{The Kraft-McMillan inequality.}
    \subfile{../subsub/1.3.1 - Kraft-McMillan}
\end{document}
